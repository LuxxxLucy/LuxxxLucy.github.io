<html>
<head>
<meta charset="UTF-8"/>
<link rel="stylesheet" href="https://luxxxlucy.github.io/projects/style_base/tufte_css_base/tufte.css">
<link rel="stylesheet" href="https://luxxxlucy.github.io/projects/style_base/framer_style_base/framer.css">

<meta name="description" content="introduction of myself">
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
    body {
        background: ffffff;
        margin-left: 0%;
        margin-right: -10%;
        width: 50%
    }

    /* main {
        max-width: 50em;
        /* max-width: 0em; */
        padding: 3em;

        /* border: medium double gray; */
        margin: 2em auto;
        /* background: lightyellow;
    } */
</style>
<link rel="stylesheet" href="./css/effect.css">
<style type="text/css">
   img { mix-blend-mode: multiply; }
  </style>
</head>

<!-- <textarea> -->
<body style="background: #f6f6f6">
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>
            mermaid.initialize({ startOnLoad: true });
    </script>

<div>
    <h1>Jialin Lu</h1>
    <p class="subtitle"></p>

    <root><p>I write this page to be used as an introduction of myself about what I’ve done and what I am working on, you can also check this page after the meeting so that you can check for relevant information.</p><p>    <a href="https://luxxxlucy.github.io/pdf/cv.pdf">CV</a><br/>    <a href="https://luxxxlucy.github.io">homepage</a><br/>    <a href="https://github.com/LuxxxLucy">github</a><br/>    <a href="https://scholar.google.ca/citations?user=BeYo3C4AAAAJ&amp;hl=en">Google Scholar</a></p><hr/><h2>Experience</h2><p><details><p><summary class="language-plaintext highlighter-rouge">click to expand</summary></p><p><p><code class="language-plaintext highlighter-rouge">2014 - 2018</code>     Zhejiang University</p><ul><li><p>BEng in Computer Science</p></li><li><p>BEng in Industrial design</p></li></ul><p><code class="language-plaintext highlighter-rouge">2018 - 2021</code>     Simon Fraser University</p><ul><li><p>MSc in Computing Science</p></li><li><p>Supervised by Martin Ester<label for="-1147350236243798686" class="margin-toggle sidenote-number"></label><input type="checkbox" id="-1147350236243798686" class="margin-toggle"/><span class="sidenote"><a href="https://scholar.google.ca/citations?user=ZYwC_CQAAAAJ&amp;hl=en">Martin Ester - Google Scholar</a></span></p></li></ul><p><code class="language-plaintext highlighter-rouge">2021 July - now</code>     Huawei Vancouver Research Center</p><ul><li><p>Promoted to Senior Engineer B in Dec 2021</p></li><li><p><p>Work with a small team working on an adaptive security engine, which provides firewall filtering service that is efficient as well as intelligent (AI/ML integrated)</p><ol><li><p>our work is used by other teams as core modules for developing a next-generation commercial product, I was specifically in charge of developing a high performance protocol stack and a module for load balancing</p></li><li><p>Also provide some side minor projects: such as an unified configuration management system, a profiler to performance/latency analysis</p></li></ol></p></li></ul></p></details></p><h2>Research: first half of master study</h2><p><details><p><summary class="language-plaintext highlighter-rouge">click to expand</summary></p><p><p>At first I do research on explaining blackbox machine learning models<label for="551495490552149580" class="margin-toggle sidenote-number"></label><input type="checkbox" id="551495490552149580" class="margin-toggle"/><span class="sidenote">Papers:<br/><strong>Interpretable drug response prediction using a knowledge-based neural network</strong> Oliver Snow, Hossein Sharifi Noghabi, Jialin Lu, Olga Zolotareva, Mark Lee, Martin Ester, KDD 2021<br/><strong>Revisit Recurrent Attention Model from an Active Sampling Perspective</strong> Jialin Lu, NeurIPS 2019 Neuro↔AI Workshop<br/><strong>An Active Approach for Model Interpretation</strong> Jialin Lu, Martin Ester, NeurIPS 2019 workshop on Human-centric machine learning (HCML2019)<br/><strong>Checking Functional Modularity in DNN By Biclustering Task-specific Hidden Neurons</strong> Jialin Lu, Martin Ester, NeurIPS 2019 Neuro↔AI Workshop<br/><strong>An Interactive Visualization Tool for Understanding Active Learning</strong> Z Wang, J Lu, O Snow, M Ester</span></p><p>However I found that precise and explainable ML might not be achieved by a purely blackbox network, certain knowledge still has to be incorporated by symbolic methods. So I start to think hybrid models</p></p></details></p><h2>Approach one to neuro-symbolic</h2><p><details><p><summary class="language-plaintext highlighter-rouge">click to expand</summary></p><p><div class="raw-html"><span class="marginnote"><img src="https://luxxxlucy.github.io/projects/2020_neural_dnf/assets/two_stage.png" width="70%"/></span></div><p>Essentially this is about a two stage model where the network produces certain high level output, which are then used by a symbolic module to produce the right output.<br/>We will use some kind of relaxation so to so to convert a non-differentiable computation into a differentiable one and then enable gradient based optimization.</p><p>This will enable to solve some tasks that involves both the perception and some reasoning.<br/><img src="https://luxxxlucy.github.io/projects/2020_neural_dnf/assets/mnist_sums_to_odd.png"/><br/><details><p><summary class="language-plaintext highlighter-rouge">click to expand: Example Result</summary></p><p><figure class="fullwidth"><label for="425134550457308824" class="margin-toggle">&#8853;</label><input type="checkbox" id="425134550457308824" class="margin-toggle"/><span class="marginnote">Inspecting the learned model</span><img src="https://luxxxlucy.github.io/projects/2020_neural_dnf/assets/mnist_sums_to_odd_result.png"/></figure></p></details></p><p>I did some work on this direction, including the master thesis<label for="-104247385516160642" class="margin-toggle sidenote-number"></label><input type="checkbox" id="-104247385516160642" class="margin-toggle"/><span class="sidenote"><a href="https://summit.sfu.ca/item/21305">Neural Disjunctive Normal Form</a></span> and a blog dedicated on how to optimize discrete parameters.<label for="-55749962849596165" class="margin-toggle sidenote-number"></label><input type="checkbox" id="-55749962849596165" class="margin-toggle"/><span class="sidenote"><a href="https://luxxxlucy.github.io/projects/2020_discrete/discrete.html">On more interesting blocks with discrete parameters in deep learning, July 2020</a></span></p><p>However it has two problems:</p><ol><li><p>about the semantic meaning of the predicates learned by the network, i.e., symbolic grounding.<br/>Sometimes, for some dataset, everything will be perfect, the NN will learn to have the right output; sometimes it becomes non-sense.<br/>I have not been able to find a good approach for this. There are simply too many problems, see for example, a simple sanity check experiment<label for="1137816196441982972" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1137816196441982972" class="margin-toggle"/><span class="sidenote">See Sec 3 of &lt;Failures of Gradient-Based Deep Learning&gt;, a tiny and well-designed experiment</span></p></li><li><p><p>about the learning algorithm.<br/>There are cases where symbolic learning algorithms (constraint-solver based, for example) can solve very easily, yet the relaxation-and-optimize-by-gradient fail miserably.<br/>The TerpreT problem is one such example, I tackled this problem in this blog.<label for="639199301917744138" class="margin-toggle sidenote-number"></label><input type="checkbox" id="639199301917744138" class="margin-toggle"/><span class="sidenote"><a href="https://luxxxlucy.github.io/projects/2020_terpret/terpret.html">Solving the TerpreT problem</a>, July 2020</span><br/>I also show how to do categorical variables and used to sample vector images.<label for="-760722952691276091" class="margin-toggle sidenote-number"></label><input type="checkbox" id="-760722952691276091" class="margin-toggle"/><span class="sidenote"><a href="https://luxxxlucy.github.io/projects/2021_terpret/index.html">Mitigating The Failures Of Gradient-Based Program Induction</a>, June 2021</span></p><div class="raw-html"><span class="marginnote"><p align="center" width=="100%"><img src="https://luxxxlucy.github.io/projects/2021_terpret/assets/circle_target.png" style="background-color:black" width="40%"/>
<img src="https://luxxxlucy.github.io/projects/2021_terpret/assets/output.gif" style="background-color:black" width="40%"/></p>
<p align="center" width=="100%"><strong>Left</strong>: target image. <strong>Right</strong>: the sampling process.
The task is given an image, find a SVG element (circile, square, etc) as well as its parameters (center, radius, etc) which after rendering approximates the image.</p></span></div></p></li></ol></p></details></p><h2>Approach 2, More recent</h2><p><details><p><summary class="language-plaintext highlighter-rouge">click to expand</summary></p><p><p>Approach two (recent project)</p><p>The target is about the synthesis of Fonts, which requires both explicit and implicit constraints.</p><p>The existing deep learning approaches tries to treat it like another image or sequence generation problem. This fails to capture the nuances in typography design. I reviewed the failure of pure neural approach in <a href="https://luxxxlucy.github.io/projects/2020_glyph/index.html">this post</a><label for="507777903980120582" class="margin-toggle sidenote-number"></label><input type="checkbox" id="507777903980120582" class="margin-toggle"/><span class="sidenote"><a href="https://luxxxlucy.github.io/projects/2020_glyph/index.html">A Review of Failure - Deep Generative Model for Chinese Fonts</a></span>.</p><p>The existing symbolic approaches try to give a rule-based generation process, yet there is not much learning here as all the specs and rules are given by human.</p><p>So here we also need a hybrid neural-symbolic approach.<br/>I am working on an approach that is a mix of program synthesis, probabilistic inference, networks (Energy-based models, to be more specific):</p><ol><li><p>Discover the ontology, relationships and constraints into a hierarchical relational graph, using program synthesis and pattern mining. This models the <strong>explicit constraints</strong></p></li><li><p>Compile into a differentiable computation graph</p></li><li><p>Learn a probabilistic model using Energy-based model, parameterizing the joint probability. This models the <strong>implicit constraints</strong></p></li><li><p>The actual generation is done by sampling, which generates samples that satisfy both <strong>explicit</strong> and <strong>implicit</strong> constraints</p></li></ol><p>Unfortunately this is not finished so I cannot show you the results now.</p><div class="raw-html"><figure class="fullwidth"><label for="380894273324860899" class="margin-toggle">&#8853;</label><input type="checkbox" id="380894273324860899" class="margin-toggle"/><span class="marginnote">Differentiable computation graph</span><img src="https://luxxxlucy.github.io/projects/2022_glyph/assets/computation_graph.svg"/></figure></div><p>Besides for the generation, we can also use this approach to do kerning, which is a sub-task of font design.</p><div class="raw-html"><p align="center" width=="100%"><img src="https://luxxxlucy.github.io/projects/2022_glyph/assets/kern_demo.gif" style="background-color:black" width="20%"/></p>
<p align="center" width=="100%">Sampling process of the kerning</p></div></p></details></p><h2>LTN demo</h2><p><details><p><summary class="language-plaintext highlighter-rouge">click to expand</summary></p><p><p>I also implemented a simple version of the LTN for the clustering mode.<label for="820775411317606572" class="margin-toggle sidenote-number"></label><input type="checkbox" id="820775411317606572" class="margin-toggle"/><span class="sidenote">see <a href="https://github.com/LuxxxLucy/ltn_demo">github</a></span></p><p>    My feeling that is there is too many tricky cases to handle.</p><div class="raw-html"><p align="center" width=="100%">    <img src="https://raw.githubusercontent.com/LuxxxLucy/ltn_demo/master/ltn_clustering_demo.gif" style="background-color:black" width="40%"/>
<p align="center" width=="100%">optimization process for a simple clustering task using logic tensor networks</p></p>
<span class="marginnote"><a href="https://github.com/LuxxxLucy/ltn_demo">my simple LTN clustering demo implemented in Julia, github</a></span></div></p></details></p></root>
</div>
</body>
</html>