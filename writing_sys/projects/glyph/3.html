<html>
<head>
<meta charset="UTF-8"/>
<link rel="stylesheet" href="https://luxxxlucy.github.io/projects/style_base/tufte_css_base/tufte.css">
<link rel="stylesheet" href="https://luxxxlucy.github.io/projects/style_base/framer_style_base/framer.css">

<meta name="description" content="">
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
    body {
        background: ffffff;
        margin-left: 0%;
        margin-right: -10%;
        width: 50%
    }

    /* main {
        max-width: 50em;
        /* max-width: 0em; */
        padding: 3em;

        /* border: medium double gray; */
        margin: 2em auto;
        /* background: lightyellow;
    } */
</style>
<link rel="stylesheet" href="./css/effect.css">
<style type="text/css">
   img { mix-blend-mode: multiply; }
  </style>
</head>

<!-- <textarea> -->
<body style="background: #f6f6f6">
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>
            mermaid.initialize({ startOnLoad: true });
    </script>

<div>
    <h1>Technical Part 1: The implicit knowledge and how to get it</h1>
    <p class="subtitle">Jialin Lu</p>

    <root><h2>Approach</h2><p>Review some works, check the prior wor ksection</p><p>1<br/>A Tutorial on Energy-Based Learning, Yann LeCun 2006 tutorial</p><p>Why EBM instead of<br/>EBM are better robustness, calibration of uncertainty<br/>YOUR CLASSIFIER IS SECRETLY AN ENERGY BASED MODEL AND YOU SHOULD TREAT IT LIKE ONE</p><p>In this paper we talk about what is the ideal way to use ML to do font kerning.</p><p>We lists some good things that we wish to obtain, and then figure out an approach to check all the boxes.</p><p>Naive way:<br/>1. train the EBM<br/>2. sample.</p><p>This is not good, the sampling process is complicated and not good (domain-shift).</p><p>A simple solution:</p><p>1. variational distribution used.<br/>2. the variational distribution helps as the noise distribution.<br/>3. Train the net and the latent parameters together by gradient.<br/>    Stops if all observation and the synthesized has score of zero.</p><h3>Improvement</h3><p>separate training and sampling.<br/>And for it we discuss:<br/>1. how to do one step<br/>    1. just the target value<br/>    2. Using auto-Decoder like. Variational inference.<br/>    3. Training by unrolling<br/>    4. By implicit layer, using deep equilibrium model-like<br/>            Joint inference and input optimization in equilibrium networks<br/>            Deep Equilibrium Architectures for Inverse Problems in Imaging<br/>2. how to do a schedule:<br/>    Patchwise Generative ConvNet: Training Energy-Based Models from a Single Natural Image for Internal Learning<br/>3. how to do inference</p><p>option 1.a.<br/>We can also just update the latent parameter together with the neural network parameters.</p><h3>Option 2</h3><p>Joint training</p><p>like in &lt;A Theory of Generative ConvNet&gt; &lt;Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet&gt;<br/>In each step, use a fixed step of Langevin dynamics to update</p><h3>Option 3</h3></root>
</div>
</body>
</html>