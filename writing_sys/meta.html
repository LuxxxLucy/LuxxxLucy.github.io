<html>
<head>
<meta charset="UTF-8"/>
<link rel="stylesheet" href="https://luxxxlucy.github.io/projects/style_base/tufte_css_base/tufte.css">
<link rel="stylesheet" href="https://luxxxlucy.github.io/projects/style_base/framer_style_base/framer.css">

<meta name="description" content="">
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
    body {
        background: ffffff;
        margin-left: 0%;
        margin-right: -10%;
        width: 50%
    }

    /* main {
        max-width: 50em;
        /* max-width: 0em; */
        padding: 3em;

        /* border: medium double gray; */
        margin: 2em auto;
        /* background: lightyellow;
    } */
</style>
<link rel="stylesheet" href="./css/effect.css">
<style type="text/css">
   img { mix-blend-mode: multiply; }
  </style>
</head>

<!-- <textarea> -->
<body style="background: #f6f6f6">
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>
            mermaid.initialize({ startOnLoad: true });
    </script>

<div>
    <h1>Meta Plan</h1>
    <p class="subtitle">Jialin Lu</p>

    <root><hr/><h2>Main statement</h2><p>1. mix use of different models for the best of all. "小孩子才做选择".<br/>2. make actual good use of the approach, I do not want to do things that only used in publication but has near-to-none use in real-world.<br/>2. make things interpretable, and most importantly, make in interpretable in a context and domain specific way.<br/>3. make the approach simple and brutal. for the sake of the bitter lesson.</p><h2>The "leading-to-" roadmap</h2><p>The structure generation.<br/>-&gt; with the Glyph part, glyph part leads to liyi.<br/>    Neural gudied search<br/>    -&gt; with program synthesis, leads to Kevin<br/>    -&gt; with receipt generation, leads to Michael<br/>-&gt; with the top-down decomposation approach<br/>    -&gt; leads to Kevin<br/>Terpret<br/>-&gt; leads to Kevin</p><h2>Outline</h2><ol><li><p>Structure generation project</p></li><ol><li><p><a href="./projects/glyph/prototype_approach.html">Simplified approach</a></p></li><li><p><a href="./projects/glyph/approach.html">Full Approach general</a></p></li><ol><li><p><a href="./projects/glyph/implicit.html">Implicit Knowledge</a></p></li><li><p><a href="./projects/glyph/explicit.html">Explicit Knowledge</a></p></li></ol><li><p>The innate reparameterzation in explicit knowledge part can also be used to do trick in do better graph generation<label for="552081553436589987" class="margin-toggle sidenote-number"></label><input type="checkbox" id="552081553436589987" class="margin-toggle"/><span class="sidenote">see Automatically Building Diagrams for Olympiad Geometry Problems</span> as well as in the TerpreT project see below</p></li></ol><ol><li><p>First and most important application, font<label for="67531535795031796" class="margin-toggle sidenote-number"></label><input type="checkbox" id="67531535795031796" class="margin-toggle"/><span class="sidenote"><a href="./projects/glyph/index.html">The glyph project</a></span></p></li><li><p>EBM neural-guided program search</p></li><li><p>General Graph generatoon. Perhaps go more neural, GFlowNet?</p></li><li><p>Move on to other structure generation. 明式家具 斗拱</p></li><li><p>Structured Text generation, post-rock?</p></li><li><p>Recipe Generation </p></li></ol><li><p>Learning (TerpreT, etc) project</p></li><ol><li><p>Initial try, including the master thesis<label for="-104247385516160642" class="margin-toggle sidenote-number"></label><input type="checkbox" id="-104247385516160642" class="margin-toggle"/><span class="sidenote"><a href="https://summit.sfu.ca/item/21305">Neural Disjunctive Normal Form</a>, Master thesis</span> and a blog dedicated on how to optimize discrete parameters.<label for="-55749962849596165" class="margin-toggle sidenote-number"></label><input type="checkbox" id="-55749962849596165" class="margin-toggle"/><span class="sidenote"><a href="https://luxxxlucy.github.io/projects/2020_discrete/discrete.html">On more interesting blocks with discrete parameters in deep learning, July 2020</a></span></p></li><li><p>Close look, the problem of learning of soft local minima, initial solution <label for="639199301917744138" class="margin-toggle sidenote-number"></label><input type="checkbox" id="639199301917744138" class="margin-toggle"/><span class="sidenote"><a href="https://luxxxlucy.github.io/projects/2020_terpret/terpret.html">Solving the TerpreT problem</a>, July 2020</span><br/>and generalize to categorical variables.<label for="-760722952691276091" class="margin-toggle sidenote-number"></label><input type="checkbox" id="-760722952691276091" class="margin-toggle"/><span class="sidenote"><a href="https://luxxxlucy.github.io/projects/2021_terpret/index.html">Mitigating The Failures Of Gradient-Based Program Induction</a>, June 2021</span></p></li><li><p>New Try:<br/>1. new framework, a general circuit compliation software, compare different optimization backend, including the four backend in TerpreT as well as probablistic circuit.<br/>2. Integrate the figure making software into this framework<br/>2. TerpreT + Mixed REAS update<br/>3. The modification of the computation graph. simolification, factorization, infer and create new link, reparameterization (x+y = 1 -&gt; y = 1-x,  two varaible can be changed to one variable), etc.</p></li></ol></ol></root>
</div>
</body>
</html>