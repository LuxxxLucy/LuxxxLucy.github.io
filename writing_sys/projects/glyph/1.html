<html>
<head>
<meta charset="UTF-8"/>
<link rel="stylesheet" href="https://luxxxlucy.github.io/projects/style_base/tufte_css_base/tufte.css">
<link rel="stylesheet" href="https://luxxxlucy.github.io/projects/style_base/framer_style_base/framer.css">

<meta name="description" content="">
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
    body {
        background: ffffff;
        margin-left: 0%;
        margin-right: -10%;
        width: 50%
    }

    /* main {
        max-width: 50em;
        /* max-width: 0em; */
        padding: 3em;

        /* border: medium double gray; */
        margin: 2em auto;
        /* background: lightyellow;
    } */
</style>
<link rel="stylesheet" href="./css/effect.css">
<style type="text/css">
   img { mix-blend-mode: multiply; }
  </style>
</head>

<!-- <textarea> -->
<body style="background: #f6f6f6">
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>
            mermaid.initialize({ startOnLoad: true });
    </script>

<div>
    <h1>The difficulty for computation: the nuance of typographic design</h1>
    <p class="subtitle">Jialin Lu</p>

    <root><h2>A Scenario of application</h2><p>Now let us first start with a scenario of application: synthesize rare or arbitrary new Chinese Characters.</p><p>Written Chinese (to some extent CJKV) uses logograhic symbols.</p><p>As contrary to languages that use an alphabet to compose a relatively small set characters into syllables and then into the words, the logographic writing system relies on a huge ton of ten thousands of characters which are already words that convey some semantic meaning.<br/>the written system gradually develops to serve as a written medium where different pronounciations/speaks can be stacked on, including the various Sinitic languages (or by Beijing's perspective, dialects) and to some lesser extent Japanese and Korean.<br/>The good thing about this system is that it democratizes the composition and invention of new words by combining several characters to an easy and common maneuver, and these new words are readily understandable by a second person without the need to lookup dictionary.</p><p><strong>However, regarding the making of font, the logogram-based system is a costly business</strong>, and has been such for a long time before the digital age.<label for="953279452896220990" class="margin-toggle sidenote-number"></label><input type="checkbox" id="953279452896220990" class="margin-toggle"/><span class="sidenote">Even though the Chinese invented movable type printing in 11-th century, the movable type system never really help the Chinese in advancing the printing industry, because the massive character set is a costly asset to maintain, unlike Gutenberg's case of alphabet which eventually boosted the printing press. An alphabet-based writing system makes much much more economical sense than a logographic system.</span><br/>The big problem of a massive set of these logograms.<br/>Each glyph needs to be implemented, the typographical consideration such as consistency enforces the designer to spend way more effort.<br/>The time and human working need for a Chinese and English font.<br/>Objectively speaking, a logogram-based system does not make much economical sense as it is easier to manage a alphabet of some 50 letters, than manage 10000 logograhic glyphs.</p><p>This disadvantageous situation does not stopped there, though, it continue to exist in the digitical age.</p><p>Font is important as ever in the digital age in the visual display of texts. Yet the main problem for Chinese fonts is that<br/>1. there aren't so many fonts<br/>2. all of the fonts are, strictly speaking, incomplete.</p><p>The main problem is the large size of the character set.<br/>A reasonable commercial font would need around 5000, while the remainning glyphs is absent, so make the font incomplete.<br/>in special cases like in specific entity, historially used yet deprecate in modern times, professional usage in historical and literature materials, as well as newly invented characters.<br/>It is very common to in real life to encounter cases where one or two character is not supported in a fontface.<br/>Very few fonts managed to provide more than 60000, only one seem to cover the entirety of all UTF standard Chinese characters (though it is the Japanese variation, Kanji, which has a lot of minor differences), and they all are very boring and dull. And that is why in a lot of cases, people will still choose to use incomplete fonts because of its aesthetic value.</p><p>This first stage's goal is to provide such a way to produce reasonably well designed rare characters from a target fontface.</p><p><span class="marginnote">&text;</span></p><p>(DaType and Intelligent Layout for Information Display: An Approach Using ConstraintsÂ and Case-based Reasoning).<br/><label for="932934966764457676" class="margin-toggle sidenote-number"></label><input type="checkbox" id="932934966764457676" class="margin-toggle"/><span class="sidenote">Maybe it is just me, but I found papers in the 90s to be more interesting and easier to read because only important/relevant references are cited.</span></p><p>DaType is great, but the main problem is where the spec comes from.<br/>For the Latin alphabet the difficulty is in partially relief because it is relatively easy to write a spec for thirty letters.<br/>It is tedious but it is still doable.</p><h2>The story of <strong>two</strong> space</h2><p>The two work flow.</p><p>First, control point to rendered to human,<br/>Now this is where the dark magic is. To illustrate how it is. and especially the Chinese font<br/>example are adjusted that the glyph should be at the center, (but not the real geometric center)<br/>we make things to have equal space/distance between them, yet the real aboslute distance are differenc.</p><p>The second, purely symbolic are.</p><p>1. make sure some basic constraints are speicfied, for example in Latin alphabet, the position of every is set to be equal.<br/>2. make sure the reused shape are consistent (except for deliberate exceptions), sometimes this is very subtle in visual effect, yet it has to be done for consistency.<br/>3. make sure the style of the points are good. Just like in programming we have a coding style, the control points should also follow some basic standard.</p><p>However,<br/>the existing either work on the perceptual realm, or in the symbolic realm</p><p>Perceptural realm. -&gt; the results are good but not detailed enough, too many things prevent we treated like a serious product</p><p>The symbolic realm, mostly have two school.<br/>1. old school. Have a good reason and theory yet do not know how the spec come from. (require constraint learning/acquisiton)<br/>2. new school. Use a model to model the joint prob of the control points. Most of them treat it as a sequence modelling.<br/>However, this is bad.<br/>Unlike the deep image prior for rendered image, statiscal modelling in the sequence is not so sense-making. As the control points are not actual sequence (it indeed has a sequence representation),<br/>yet the points forms some hierarchy and sets.<br/>Also the sequence for a equivalent shape is not unique, it is easy to alter the order the control points,<br/>we can even modify the cubic and quadratic bezier curve and make a large set of sequence that has exactly the same rendering result.<br/>or the issue of overlapping.</p><figure class="fullwidth"><label for="25249864736772020" class="margin-toggle">&#8853;</label><input type="checkbox" id="25249864736772020" class="margin-toggle"/><span class="marginnote">an figure</span><img src="./asset/two_space.svg"/></figure><p>We want to do better in a hybrid model.</p><p>inducing constraints that are about the relation of shapes cannot be handled without a perceptual component</p><p>Machine aided Font design require to handle two nuances, one for consistency and one for delusion.<br/>Deep learning is good yet as we will argue provides limited help</p><h2>Approach</h2><p>Review some works, check the prior wor ksection</p><p>Why EBM instead of<br/>EBM are better robustness, calibration of uncertainty</p><p>YOUR CLASSIFIER IS SECRETLY AN ENERGY BASED MODEL AND YOU SHOULD TREAT IT LIKE ONE</p><p>sampling based learning is good, yet it is costly.</p><p>neural approximate inference is also good in theory, but the problem is that the parameters for each character has different meaning and different size. It would still be doable, yet then this becomes unnecessarily useless.</p><p>In this paper we talk about what is the ideal way to use ML to do font kerning.<br/>We lists some good things that we wish to obtain, and then figure out an approach to check all the boxes.</p><p>Here we propose a hybrid, most part symbolic, approach, following the idea of "design by example"(cite model by example and shape by example) and case-based reasoning.<br/>We also want to make a dark magic model (DNN, Probabilistic inference) as small as possible.<br/>DNN is also not easy to train (underfitting and overfitting hurs especially so in our case)<br/>(also why we want to use case-based reasoning.)</p><p>We will utlize DNN, but not for its  predictive performance or magic pwoer, but for a good in-hand approximation of human visual processin.</p><p>DNN is also so good at compositional/combinatorial issues. And when we have a large set of glyphs, training a DNN or anything probabilistic modelling would be very expensive.<br/>So we are using case-based reasoning.</p><p>We will start with kerning, then spacing,, consistency and exception mining, decomposition and lastly whole synthesis of glyphs.</p><p>Human designers take hard efforts to design and a lot of them is hard to talk with language<br/>.<br/>Here we present what we can do, we start with the fact that a reasonably well-designed font glyph represents human designers' dedicated work and is thus optimal according to the designer's standard.</p><p>Here we do not wish to automate the whole process, yet just providing some routines to help the font design process.</p><p>What we offer:</p><ul><li><p>for <em>implicit</em> constraints, neural network. Find a flexible framework that is able to modify and extend.</p></li><li><p>for <em>explicit</em> constraints, data mining and discover. Discover constraints and design space hierarchy to automatically learn from data. Techniques learned from Program synthesis.</p></li><li><p>These two can be unfused together.</p></li></ul><p>Explicit model, good in theory, but bad in its lack of visual knowledge, to it the perceptual difference does not mean anything. Symbolic approaches need to be grounded with visual information.<br/>Blackbox model, good at some fancy prototype, yet lack the precise detail. Neural approaches need to also be able to take the explicit constraints into consideration.</p><p>Font Design is about rational consistency and visual delusions.<br/>None of the two approach alone can capture them both.</p><p>Rational consistency: the font is designed in a rational way, there are rules to enforce, basic unit, and the hierarchy of constraints.</p><p>Visual delusion: the dark art of designer, not always describable by language alone. While in theory when we want to make something be equal space, the designers deliberately design it inequally, so that visually it will be equal to human's eye.<br/>When we want to a shape to be centered in the midpoint, the designers deliberately design it not in the exact midpoint so to make it visually so.</p><p>This is about the art of consistency and delusion.</p><p>Neither purely symbolic nor statistical approach can make it work. And that is why for decades people tried yet fail.</p><p>This problem is serious in all visual design, yet Even more so in fonttype design:<br/>because it is so subtle, layman ignores; but fontype is used everywhere to render text, the tiniest subtlty becomes noticable when layman stares at a whole paragraph of rendered text.</p><p>Getting the AI to do design is hard:<br/>1. symbolic approaches, good at specifying the constraints, rules and exceptions, good at case based reasoning. It is hard to induce such constriants from data. (constraint learning/acquisition). Besides, what human consdier to be a rule over shapes must have a visual perception prossing, and this is where it break, symbolic approaches cannot handle perception<br/>(todo: explain it</p><p>)<br/>2. neural approaches, impressive result recent years, good discovery (CNN actually approximates visual processing, deep image prior). Yet difficult to enforce consistent generalization and compositional generalization<br/>(todo: explain it<br/>This is important when you need to model the relationship, just like in recommendation systems you always need to use context (collaborative filtering) to give a very precise result, otherwise just use features solely about the user but without the important context is bound to fail.</p><p>That being said, sueprvised prediction or reinforcement learning did not consider the consistency problem very thoroughly. It can be used in this way, yet it still have to implement some symbolic check, as in the case of neural guided search in program synthesis</p><p>Another reason is poor modelling, the application actually gives us rich information, yet just a predictive model is just not going to fully utilize such information<br/>)</p><p>A toolkit for assistance of font design.</p><p>includnig analysis, autocomplete, version control/conflict handling.Steps</p><ul><li><p>DaType generation tool</p></li><li><p>Commit single change</p></li><li><p></p></li></ul><p>Good old things</p><ul><li><p>DaType</p></li></ul><p>New things</p><ul><li><p>Fonttool</p></li><li><p>Glyphs 3</p></li></ul><p>lalalaaldsdsa<span class="marginnote">&text;</span>lalalaaldsdsa<label for="840858626114532943" class="margin-toggle sidenote-number"></label><input type="checkbox" id="840858626114532943" class="margin-toggle"/><span class="sidenote">this is a numbered note</span></p></root>
</div>
</body>
</html>